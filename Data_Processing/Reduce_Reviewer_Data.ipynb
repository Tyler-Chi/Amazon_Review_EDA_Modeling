{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "445a271d-4660-4a7b-9013-49b746ef99f6",
   "metadata": {},
   "source": [
    "# Finalize reducing of the reviewer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1bc741a-9a3d-4b6d-b293-2907197a0244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "pd.options.mode.chained_assignment = None  # default='warn' # this is needed because setting temporary column value on chunk\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "cpu_count = multiprocessing.cpu_count() # 16 cores\n",
    "cpu_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9006b43c-9bbf-418d-b5ae-b2db20f2af36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 183M rows after the initial map-reduce\n",
    "num_rows = 183095823\n",
    "chunksize = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3c6a7d2-4ba1-4932-ae8f-c03ae50e9576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# previously saved the set of unique reviewer ID's\n",
    "with open('./reviewer_ids', 'rb') as f:\n",
    "    reviewer_id_full_set = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "415afc00-84b6-4c1d-8314-4a9f105c2bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main bottleneck is keeping all 40m reviewer information in memory\n",
    "# therefore need to go through, and just consolidate one quarter at a time\n",
    "reviewer_id_chunks = np.array_split(list(reviewer_id_full_set), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5afb0b88-583e-437a-af0e-09adb2bd46c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['reviewerID','1*','2*','3*','4*','5*']\n",
    "# initialize empty csv, which will be populated\n",
    "empty_df = pd.DataFrame(columns=[columns])\n",
    "empty_df.to_csv(\"./Intermediate_Datasets/consolidated_reviewer_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d6861a-63fb-4db4-93b1-1ad4e04cceda",
   "metadata": {},
   "source": [
    "Reducing was definitely the most computationally expensive part of the data processing. Because there are ~43m reviewers, it is difficult to keep all of them in memory, while going through the mapped-reviews and updating the source reviewers accordingly.\n",
    "\n",
    "The workaround I did was to split the reviewer id's into splits (reviewer_id_chunks), and process only process one split at a time. This way, only ~10m rows of reviewers needs to be kept in memory at any given point.\n",
    "\n",
    "Multiprocessing was used to further decrease the run time. Each core handled a different portion of the reviewer_id_chunk. I did this because I noticed that updating large pandas dataframes is very slow.\n",
    "\n",
    "This entire process took ~3 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7371e604-e547-4a29-855c-b9b30be72299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# both df's essentially represent reviewer_df's in the same form\n",
    "# just need to use the chunk_df to update the persisted_df, according to reviewer_id\n",
    "def worker(chunk_df, persisted_df):\n",
    "    # first do one more reduction, prior to iterating and updating storage df's\n",
    "    chunk_df = chunk_df.groupby('reviewerID').sum(numeric_only=True).reset_index()\n",
    "    persisted_df = pd.concat([chunk_df,persisted_df]).groupby('reviewerID').sum().reset_index()\n",
    "    return persisted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a989da63-7a7d-44a5-a768-92780dc43ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Q0: : 184000000it [50:00, 61331.24it/s]                                                                        \n",
      "Q1: : 184000000it [51:59, 58974.97it/s]                                                                        \n",
      "Q2: : 184000000it [43:37, 70295.53it/s]                                                                        \n",
      "Q3: : 184000000it [43:30, 70487.54it/s]                                                                        \n"
     ]
    }
   ],
   "source": [
    "for reviewer_id_chunk_idx, reviewer_id_chunk in enumerate(reviewer_id_chunks):\n",
    "    # this block essentially goes through the entire dataset, and looks for information related\n",
    "    # to this reviewer_id_chunk. these rows are then consolidated together in memory (reviewer_df_list)\n",
    "    # at the end of going through the entire dataset, append it to the .csv file, then move onto\n",
    "    # the next reviewer_id_chunk\n",
    "    reviewer_id_cpu_splits = np.array_split(list(reviewer_id_chunk), cpu_count)\n",
    "    group_sets = {}\n",
    "    \n",
    "    for index, reviewer_id_list in enumerate(reviewer_id_cpu_splits):\n",
    "        group_sets[index] = set(reviewer_id_list)\n",
    "\n",
    "    reviewer_df_list = []\n",
    "    for group_id in group_sets:\n",
    "        reviewer_df_list.append(pd.DataFrame(columns=columns))\n",
    "\n",
    "    tqdm._instances.clear()\n",
    "\n",
    "    with tqdm(total=num_rows) as pbar:\n",
    "        pbar.set_description(f\"Q{reviewer_id_chunk_idx}\")\n",
    "        for chunk in pd.read_csv(\n",
    "            \"./Intermediate_Datasets/reviewer_data_non_reduced.csv\",\n",
    "            chunksize=chunksize\n",
    "        ):      \n",
    "            # define tasks to send to processes\n",
    "            task_list = []\n",
    "            for group_id, group_set in group_sets.items():\n",
    "                relevant_df = chunk.loc[chunk['reviewerID'].isin(group_set)]\n",
    "                task_list.append((\n",
    "                    relevant_df,\n",
    "                    reviewer_df_list[group_id]\n",
    "                ))\n",
    "\n",
    "            pool = multiprocessing.Pool(processes = cpu_count)\n",
    "            reviewer_df_list = pool.starmap(worker, task_list)\n",
    "            pool.close()\n",
    "            pbar.update(chunksize) \n",
    "    \n",
    "    # combine the list of review_df_list, and commit it to the csv. then move onto the next reviewer_id_chunk\n",
    "    combined_reviewer_df = pd.concat(reviewer_df_list, axis=0)\n",
    "    combined_reviewer_df.to_csv(\"./Intermediate_Datasets/consolidated_reviewer_data.csv\", mode=\"a\", header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
